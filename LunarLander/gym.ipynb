{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/bonzo_yang/github/openai-gym/LunarLander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 543 # Do not change this\n",
    "def check_cuda():\n",
    "    import torch\n",
    "    \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    gpus = torch.cuda.device_count()\n",
    "    print(f\"{gpus} GPUS are available, set device to {device}\")\n",
    "    return device\n",
    "    \n",
    "\n",
    "def fix(env, seed):\n",
    "    env.reset(seed=seed)# env.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.use_deterministic_algorithms(True) # torch.set_deterministic(True)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入 OpenAI 的 gym，並建立一個 [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) 環境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPUS are available, set device to cuda:0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "check_cuda()\n",
    "fix(env, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什麼是 Lunar Lander？\n",
    "\n",
    "“LunarLander-v2” 這個環境是在模擬登月小艇降落在月球表面時的情形。\n",
    "這個任務的目標是讓登月小艇「安全地」降落在兩個黃色旗幟間的平地上。\n",
    "> Landing pad is always at coordinates (0,0).  \n",
    "> Coordinates are the first two numbers in state vector.  \n",
    "\n",
    "![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
    "\n",
    "所謂的「環境」其實同時包括了 agent 和 environment。\n",
    "我們利用 `step()` 這個函式讓 agent 行動，而後函式便會回傳 environment 給予的 observation/state（以下這兩個名詞代表同樣的意思）和 reward。\n",
    "\n",
    "### Observation / State\n",
    "\n",
    "首先，我們可以看看 environment 回傳給 agent 的 observation 究竟是長什麼樣子的資料："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Box(8,)` 說明我們會拿到 8 維的向量作為 observation，其中包含：垂直及水平座標、速度、角度、加速度等等，這部分我們就不細說。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "而在 agent 得到 observation 和 reward 以後，能夠採取的動作有：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Discrete(4)` 表示 agent 有 4 種 action 可以執行\n",
    "`0` - Do nothing  \n",
    "`1` - Fire left engine (向左加速)\n",
    "`2` - Fire down engine (向下加速)\n",
    "`3` - Fire right engine (向右加速)\n",
    "\n",
    "接下來，我們將嘗試讓 agent 跟 environment 互動。  \n",
    "在採取任何 action 之前，我們建議調用 `reset()` 函式來重置環境。此外，此函式將 return 環境的初始狀態。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00708485  1.4177319   0.7175924   0.30272722 -0.00820268 -0.16254565\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "initial_state, initial_info = env.reset()\n",
    "print(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然後，我們嘗試從 agent 的 action space  中獲取隨機 action。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "random_action = env.action_space.sample()\n",
    "print(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "此外，我們可以利用 `step()` 使 agent 根據隨機選擇的 `random_action` 進行操作。`step()` 函數將回傳四個值：  \n",
    "- `observation (object)`: 這將是環境的 :attr: `observation_space` 的一個元素。例如，這可能是一個包含某些對象的位置和速度的 numpy tuple。  \n",
    "- `reward (float)`: 是否達到 `terminal state` (根據任務的 MDP 定義)。在這種情況下，進一步的 `step()` 呼叫可能會回傳未定義的結果。  \n",
    "- `terminated (bool)`: 採取 action 後返回的 reward。  \n",
    "- `truncated (bool)`: 是否滿足 MDP 範圍之外的 truncation 條件。通常是因為 timelimit 造成的，但也可用於表示 agent 是否超出物理範圍。可用在達到 `terminal state` 之前就提前終止 episode。  \n",
    "- `info (dictionary)`: `info` 包含輔助診斷信息 (有助於 debug、learning 和 logging)。例如，這可能包含：描述 agent 性能狀態的指標、變數，這都隱藏在 observation 或小 reward (最後會累積成總 reward)。它還包含可以分辨 truncation 跟 termination 的資訊，但這種回傳兩個 boolean 值的作法，將不再受到青睞，並且在後續的版本將被棄用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, terminated, truncation, info = env.step(random_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(truncation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "\n",
    "而「環境」給予的 reward 大致是這樣計算：\n",
    "- 小艇墜毀得到 -100 分\n",
    "- 小艇在黃旗幟之間成功著地則得 100~140 分\n",
    "- 噴射主引擎（向下噴火）每次 -0.3 分\n",
    "- 小艇最終完全靜止則再得 100 分\n",
    "- 小艇每隻腳碰觸地面 +10 分\n",
    "\n",
    "> Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.  \n",
    "> If lander moves away from landing pad it loses reward back.  \n",
    "> Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points.  \n",
    "> Each leg ground contact is +10.  \n",
    "> Firing main engine is -0.3 points each frame.  \n",
    "> Solved is 200 points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.6297025784652124\n"
     ]
    }
   ],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent\n",
    "最後，在開始 train 之前，來看看 random agent 能否成功登陸月球："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "img = plt.imshow(env.render()) # img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncation, info = env.step(action)\n",
    "\n",
    "    img.set_data(env.render()) # img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "現在來搭建一個簡單的 policy network。\n",
    "我們預設模型的輸入是 8-dim 的 observation，輸出則是離散的四個動作之一："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "\n",
    "    def forward(self, state):\n",
    "        out = nn.LeakyReLU(0.05)(self.fc1(state))\n",
    "        out = nn.LeakyReLU(0.05)(self.fc2(out))\n",
    "        out = F.softmax(self.fc3(out), dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來，搭建一個簡單的 agent，並搭配上方的 policy network 來採取行動。  \n",
    "這個 agent 能做到以下幾件事：  \n",
    "- `learn()`：從記下來的 log probabilities 及 rewards 來更新 policy network。  \n",
    "- `sample()`：從 environment 得到 observation 之後，利用 policy network 得出應該採取的行動。  \n",
    "而此函式除了回傳抽樣出來的 action，也會回傳此次抽樣的 log probabilities。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class PolicyGradientAgent():\n",
    "    \n",
    "    def __init__(self, network, lr=0.001):\n",
    "        self.network = network\n",
    "        self.optimizer = optim.SGD(self.network.parameters(), lr=lr)\n",
    "         \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "    def learn(self, log_probs, rewards):\n",
    "        loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def sample(self, state):\n",
    "        action_prob = self.network(torch.FloatTensor(state))\n",
    "        action_dist = Categorical(action_prob)\n",
    "        action = action_dist.sample()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        action = action.item() # only one element tensors can be converted to Python scalars\n",
    "        return action, log_prob \n",
    "\n",
    "    def save(self, PATH): # You should not revise this\n",
    "        agent_dict = {\n",
    "            \"network\" : self.network.state_dict(),\n",
    "            \"optimizer\" : self.optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(agent_dict, PATH)\n",
    "\n",
    "    def load(self, PATH): # You should not revise this\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.network.load_state_dict(checkpoint[\"network\"])\n",
    "        #如果要儲存過程或是中斷訓練後想繼續可以用喔 ^_^\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後，建立一個 network 和 agent，就可以開始進行訓練了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練 Agent\n",
    "\n",
    "現在我們開始訓練 agent。\n",
    "透過讓 agent 和 environment 互動，我們記住每一組對應的 log probabilities 及 reward，並在成功登陸或者不幸墜毀後，回放這些「記憶」來訓練 policy network。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1382407066.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [17], line 17\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# class RLTrainer:\n",
    "#     def __init__(self, agent, env, hp={\"episode_per_batch\":5, \"num_batch\":400}):\n",
    "#         self.agent = agent\n",
    "#         self.env = env\n",
    "#         self.hp = hp # hyper parameter\n",
    "        \n",
    "#     def shape_check(l):\n",
    "#         dim = [] \n",
    "#         if len(l) > 0:\n",
    "#             if hasattr(l[0], \"shape\"):\n",
    "#                 dim += [_ for _ in l[0].shape]\n",
    "            \n",
    "#         dim = tuple(dim)\n",
    "#         return dim\n",
    "\n",
    "#     def train():\n",
    "#         self.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_digits(n):\n",
    "    import math\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    elif n > 0:\n",
    "        return int(math.log10(n))+1\n",
    "    else:\n",
    "        return int(math.log10(n))+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 252, episode:4, steps      129, rewards.shape: torch.Size([659]), log_probs.shape: torch.Size([659])\r"
     ]
    }
   ],
   "source": [
    "agent.network.train()  # 訓練前，先確保 network 處在 training 模式\n",
    "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
    "NUM_BATCH = 400        # 總共更新 400 次\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "for batch in prg_bar:\n",
    "\n",
    "    log_probs, rewards = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "\n",
    "    # 蒐集訓練資料\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        if batch in (197, 252):\n",
    "           img = plt.imshow(env.render())\n",
    "\n",
    "        total_reward, total_step = 0, 0\n",
    "        while True:\n",
    "\n",
    "            action, log_prob = agent.sample(state) # at , log(at|st)\n",
    "            next_state, reward, terminated, truncation, info = env.step(action)\n",
    "\n",
    "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "            rewards.append(reward) #改這裡\n",
    "            # ! 重要 ！\n",
    "            # 現在的reward 的implementation 為每個時刻的瞬時reward, 給定action_list : a1, a2, a3 ......\n",
    "            #                                                       reward :     r1, r2 ,r3 ......\n",
    "            # medium：將reward調整成accumulative decaying reward, 給定action_list : a1,                         a2,                           a3 ......\n",
    "            #                                                       reward :     r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,r3+0.99*r4+0.99^2*r5+ ......\n",
    "            # boss : implement DQN\n",
    "            # if done:\n",
    "\n",
    "\n",
    "            # action = env.action_space.sample()\n",
    "            # observation, reward, terminated, truncation, info = env.step(action)\n",
    "\n",
    "            if batch in (197, 252):\n",
    "                img.set_data(env.render()) # img.set_data(env.render(mode='rgb_array'))\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "                msg = f\"batch {batch:{int_digits(NUM_BATCH)}d}, episode:{episode}, steps {total_step: 8d}, rewards.shape: {torch.from_numpy(np.array(rewards)).size()}, log_probs.shape: {torch.stack(log_probs).size()}\"\n",
    "                print(msg, end='\\r')\n",
    "\n",
    "\n",
    "            if terminated:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                break\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    # 紀錄訓練過程\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # 更新網路\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # 將 reward 正規標準化\n",
    "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
    "\n",
    "\n",
    "    msg = f\"batch {batch:{int_digits(NUM_BATCH)}d}, episode:{episode}, steps {total_step: 8d}, rewards.shape: {torch.from_numpy(rewards).size()}, log_probs.shape: { torch.stack(log_probs).size()}\"\n",
    "    print(msg, end='\\r')\n",
    "    # print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "    # print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('lunarlander-hVTIrlPl-py3.9': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5063266e172670fe772e5cd871bf87ea1af57b542ed25e1d60af1d810675a6a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
